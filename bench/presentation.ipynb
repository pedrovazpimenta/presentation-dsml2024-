{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import utils\n",
    "import slm\n",
    "\n",
    "\n",
    "MODEL = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "In this example, we'll train a generative text model from scratch using PyTorch. The goal of our model is to generage sequences of 20 characters containing only parentheses and this must count as a balanced expression, i.e., the number of opening and closing parentheses must be equal and they must be correctly nested. For example, `()(())()` is a balanced expression, but `())(` is not.\n",
    "\n",
    "### Generating the dataset\n",
    "\n",
    "So the first step is to build out dataset. We'll generate all the possible expressions of size 20 algorithmically. We're also programming a function to check if a given expression is balanced to later evaluate the results from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "opening = [\"(\"]\n",
    "closing = [\")\"]\n",
    "vocalulary = opening + closing + [\"E\", \"\"]\n",
    "\n",
    "\n",
    "def sequence_verifyer(sequence):\n",
    "    stack = []\n",
    "    for token in sequence:\n",
    "        if token in opening:\n",
    "            stack.append(token)\n",
    "        elif token in closing:\n",
    "            if len(stack) == 0:\n",
    "                return False\n",
    "            if closing.index(token) != opening.index(stack.pop()):\n",
    "                return False\n",
    "    return len(stack) == 0\n",
    "\n",
    "\n",
    "def gen_parentheses_util(n, open, close, s, ans):\n",
    "\n",
    "    if open == n and close == n:\n",
    "        ans.append(s)\n",
    "        return\n",
    "\n",
    "    if open < n:\n",
    "        gen_parentheses_util(n, open + 1, close, s + \"(\", ans)\n",
    "\n",
    "    if close < open:\n",
    "        gen_parentheses_util(n, open, close + 1, s + \")\", ans)\n",
    "\n",
    "\n",
    "def generate_balanced_expressions(n):\n",
    "\n",
    "    ans = []\n",
    "    if n > 0:\n",
    "        gen_parentheses_util(n, 0, 0, \"\", ans)\n",
    "    return ans\n",
    "\n",
    "\n",
    "nice_one = generate_balanced_expressions(10)\n",
    "corpus = [item + \"E\" for item in nice_one]\n",
    "tokens = vocalulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the model and parameters\n",
    "\n",
    "We'll  choose a very simple model based in a single Transformer Decoder. We'll also use a very small size of embeddings and number of dense layers. In total, our model will have 12124 parameters, which makes it very very small and easy to train with a CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 4\n",
    "d_model = 24\n",
    "num_heads = 4\n",
    "ff_hidden_layer = 8 * d_model\n",
    "dropout = 0.1\n",
    "num_layers = 3\n",
    "context_length = 24\n",
    "vocab_size = len(tokens)\n",
    "epochs = 5\n",
    "batch_size = 1\n",
    "model_best_name = \"model_best.pkl\"\n",
    "model_last_name = \"model_last.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12124"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_transformer_model = slm.TransformerDecoder(\n",
    "    vocab_size, d_model, num_heads, ff_hidden_layer, dropout\n",
    ")\n",
    "slm.count_parameters(small_transformer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = slm.ExpressionDataset(corpus, tokens)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "criterion = nn.NLLLoss(ignore_index=tokens.index(\"\"))\n",
    "optimizer = optim.Adam(small_transformer_model.parameters(), lr=0.001)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = small_transformer_model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    current_loss = 1000000000\n",
    "\n",
    "    for batch in tqdm(dataloader):\n",
    "        input_indices, target_indices = batch\n",
    "        input_indices = input_indices.transpose(0, 1).to(device).long()\n",
    "        target_indices = target_indices.transpose(0, 1).to(device).long()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_indices)\n",
    "\n",
    "        loss = criterion(\n",
    "            output.view(-1, output.size(-1)),\n",
    "            target_indices.reshape(-1),\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    torch.save(model, model_last_name)\n",
    "    if total_loss < current_loss:\n",
    "        current_loss = total_loss\n",
    "        torch.save(model, model_best_name)\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model\n",
    "\n",
    "With the train done, we can use the following function to generate new expressions using a small part as seed. We can also control parameters like the temperature of the softmax function to generate more or less random expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(())(((((())()))()))\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, seed_text, max_length, tokens, temperature=1.0):\n",
    "    model.eval()\n",
    "\n",
    "    device = torch.device(\"cpu\")\n",
    "    generated_sequence = [tokens.index(char) for char in seed_text]\n",
    "\n",
    "    for _ in range(max_length - len(seed_text)):\n",
    "        input_tensor = (\n",
    "            torch.tensor(generated_sequence).unsqueeze(1).to(device).long()\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            logits = output[-1, 0, :] / temperature\n",
    "            probabilities = F.softmax(logits, dim=-1).cpu().numpy()\n",
    "\n",
    "            next_token = np.random.choice(len(tokens), p=probabilities)\n",
    "\n",
    "        generated_sequence.append(next_token)\n",
    "\n",
    "        if tokens[next_token] == \"E\":\n",
    "            break\n",
    "\n",
    "    return \"\".join(tokens[idx] for idx in generated_sequence)\n",
    "\n",
    "\n",
    "example = generate_text(model, \"(())\", 24, tokens, temperature=0.8)[:-1]\n",
    "print(example)\n",
    "print(sequence_verifyer(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Train your model and evaluate its performance by creating multiple seeds, generating multiple outputs for each seed, and calculating how many are correctly balanced.\n",
    "\n",
    "**Homework:** Try to improve the model's performance by changing the model hyperparameters, size, training scheme, etc.\n",
    "\n",
    "**Challenge:** Try training a LSTM model with similar size for the same task and compare the performance.\n",
    "\n",
    "**Challenge:** Try increasing the model and dataset size by adding more symbols, such as `[]` amd `{}`. See is the performance is maintained. You can also try different tokenization schemes, such as using specific groups of characters as tokens, for that, you can try the byte-pair encoding algorithm, which is used in Large Language Models like the GPT family."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Engineering\n",
    "\n",
    "### 1) Explain clearly the task;\n",
    "\n",
    "That's the most important thing. The model needs to understand what it is supposed to do. And when the user is not aware of that, we need to compensate for it with a good system prompt and supporting instructions.\n",
    "\n",
    "### 2) Provide examples;\n",
    "\n",
    "This practice is also known as \"few shot learning\" (as opposed to \"zero-shot\" where no example is given). It helps the model to understand the task better, inclusind input and output patterns.\n",
    "\n",
    "### 3) Provide a baseline;\n",
    "\n",
    "This is a good practice to help the model understand what is expected from it. So, if we can't provide real examples, we can at least provide a blueprint of what the output should look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment of the statement \"I am feeling great today.\" is positive.\n"
     ]
    }
   ],
   "source": [
    "# Examples for 1,2 and 3\n",
    "# What not to do:\n",
    "\n",
    "system_message = \"You are a helpful assistant. Here is a question, now provide a correct and quick answer.\"\n",
    "# no example, so the answer is not clear or correct\n",
    "question = \"\"\"Evaluate the sentiment of the following statement: \"I am feeling great today.\" \"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": question},\n",
    "]\n",
    "\n",
    "response = utils.get_chat_completionprompt(MODEL, messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "# Examples for 1,2 and 3\n",
    "# What not to do:\n",
    "system_message = \"You are a helpful assistant. Here is a question, now provide a correct and quick answer.\"\n",
    "# examples\n",
    "example_question_1 = \"Evaluate the sentiment of the following statement: 'I am feeling great today.'\"\n",
    "example_response_1 = \"Positive\"\n",
    "example_question_2 = \"Evaluate the sentiment of the following statement: 'I am feeling terrible today.'\"\n",
    "example_answer_2 = \"Negative\"\n",
    "example_question_3 = \"Evaluate the sentiment of the following statement: 'I am feeling okay today.'\"\n",
    "example_answer_3 = \"Positive\"\n",
    "question = \"Evaluate the sentiment of the following statement: 'I am feeling outstanding today.'\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": example_question_1},\n",
    "    {\"role\": \"assistant\", \"content\": example_response_1},\n",
    "    {\"role\": \"user\", \"content\": example_question_2},\n",
    "    {\"role\": \"assistant\", \"content\": example_answer_2},\n",
    "    {\"role\": \"user\", \"content\": example_question_3},\n",
    "    {\"role\": \"assistant\", \"content\": example_answer_3},\n",
    "    {\"role\": \"user\", \"content\": question},\n",
    "]\n",
    "\n",
    "response = utils.get_chat_completionprompt(MODEL, messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Explain the output exptected;\n",
    "\n",
    "Together with the previous one, this practice helps the model to understand what is expected from it, but in this case we use words instead of examples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# Exanmple for 4\n",
    "# What not to do:\n",
    "system_message = \"You are a helpful assistant. Here is a question, now provide a correct and quick answer.\"  # not provising instructions for output\n",
    "question = \"What is the capital of France?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": question},\n",
    "]\n",
    "\n",
    "response = utils.get_chat_completionprompt(MODEL, messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brasília\n"
     ]
    }
   ],
   "source": [
    "# Exanmple for 4\n",
    "# What to do:\n",
    "system_message = \"You are a helpful assistant. Here is a question, now provide a correct and quick answer. We expect a single word answer, for example: Question: What is the capital of France? Answer: Paris.\"\n",
    "question = \"What is the capital of Brazil?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": question},\n",
    "]\n",
    "\n",
    "response = utils.get_chat_completionprompt(MODEL, messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Provide very few options;\n",
    "\n",
    "When the model is doing classification tasks, it's better to provide very few and distinct options for it to choose from. Otherwise, it may get confused and the results might be very inconsistent. Even changing the order of the options can chance the output, so we need to be very careful with how we choose and display them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress Report\n"
     ]
    }
   ],
   "source": [
    "# example for 5\n",
    "# What not to do:\n",
    "system_message = \"You are a helpful assistant. You're given text and you must choose the most appropriate classification from the options provided and only them: Status Report, Progress Report, Incident Report, Summary Report, Blocker Report, Personal Report, Issue Report.\"\n",
    "text = \"The project is going well. We have completed the first phase and are now moving on to the second phase. We are on track to meet the deadline.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": text},\n",
    "]\n",
    "\n",
    "response = utils.get_chat_completionprompt(MODEL, messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Report\n"
     ]
    }
   ],
   "source": [
    "# example for 5\n",
    "# What to do: provide very few options that are very distinct from each other\n",
    "system_message = \"You are a helpful assistant. You're given text and you must choose the most appropriate classification from the options provided and only them: Project Report, Personal Report, Incident Report.\"\n",
    "text = \"The project is going well. We have completed the first phase and are now moving on to the second phase. We are on track to meet the deadline.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": text},\n",
    "]\n",
    "\n",
    "response = utils.get_chat_completionprompt(MODEL, messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Make the model adopt a persona;\n",
    "\n",
    "This may look very silly, but it's something that indeed improves a lot the results, as observed in multiple independent experiments with multiple models. This role-play tactics may help a lot in the model's performance and that includes not only a persona, but also politeness, empathy, positive remarks and so on.\n",
    "\n",
    "### 7) Clearly delimit the instructions, context and the prompt;\n",
    "\n",
    "Using line breaks, paragraphs, bullet points, etc. to separate the instructions from the context and the prompt is a good practice to help the model understand what is expected from it.\n",
    "\n",
    "### 8) Provide steps to follow during the task completion;\n",
    "\n",
    "Asking the model to provide each step of the solution is a good practice and can improve a lot the results. It's like breaking the task into smaller tasks and asking the model to solve them one by one. The downside is that it may take longer to get the final result, and sometimes we need to parse the output to get the final solution, which may not be easy (but we can also ask the model to do that).\n",
    "\n",
    "### 9) Ask the model to cite the references provided;\n",
    "\n",
    "This keeps the model on topic and prevents hallucination.\n",
    "\n",
    "### 10) Use intent classification models to classify the intent of the user and select the appropriate prompt;\n",
    "\n",
    "A good practice is to have multiple sets of prompts for different types of tasks, and use a model to classify the intent of the user and select the appropriate prompt for that intent. Sometimes we can even use different models as well, depending on the task. That's why it's important to have a system capable of evaluating the performance of the models over time and select the best one for each task.\n",
    "\n",
    "### 11) Summarize previous conversations;\n",
    "\n",
    "For conversational models, it's a good practice to summarize the previous interactions, so the model can remember what was being discussed and keep the context and not having to use the whole conversation history as context, as this not only can get very expansive but also may include noise that can confuse the model. The models themselves can be used for summarization and are pretty good at it.\n",
    "\n",
    "### 12) Ask the model to use inner monologue or to provide each step for the solution;\n",
    "\n",
    "This is a step further from 8, and includes that together with multiple back and forths between the model and a fixed set of prompts or even another model, to get to the final solution. This is a good practice to get more complex tasks done, but it may take a lot of time and resources.\n",
    "\n",
    "### 13) Ask the model to evaluate the solution and see if anything was missed;\n",
    "\n",
    "Another improvement over the last point is to use the same model or another one to evaluate the solution provided by the model, and see if anything was missed. And, in that case, to provide that missing information and the final solution.\n",
    "\n",
    "### 14) Evaluate the output of the model;\n",
    "\n",
    "Some outputs can be evaluated automatically by algorithms or other Machile Learning models. When that's possible, it's a great practice, as it can provide feedback to the model and help it improve over time or even come to the correct answer with more tries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To retrieve a table containing each decade along with the most popular male and female names, you can use a SQL query that combines the necessary filtering, aggregation, and ordering functions. Below is an example SQL query that achieves this:\n",
      "\n",
      "```sql\n",
      "WITH ranked_names AS (\n",
      "    SELECT\n",
      "        decade,\n",
      "        name,\n",
      "        gender,\n",
      "        quantity,\n",
      "        ROW_NUMBER() OVER (PARTITION BY decade, gender ORDER BY quantity DESC) as rank\n",
      "    FROM my_table\n",
      ")\n",
      "\n",
      "SELECT\n",
      "    decade,\n",
      "    MAX(CASE WHEN gender = 'female' THEN name END) AS most_popular_female_name,\n",
      "    MAX(CASE WHEN gender = 'male' THEN name END) AS most_popular_male_name\n",
      "FROM ranked_names\n",
      "WHERE rank = 1\n",
      "GROUP BY decade\n",
      "ORDER BY decade;\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "1. **CTE (Common Table Expression)**: The `ranked_names` CTE ranks names for each `decade` and `gender` based on their `quantity` using the `ROW_NUMBER()` function.\n",
      "2. **Filtering**: We filter the results in the outer `SELECT` statement to include only the names that are ranked #1 for their respective genders.\n",
      "3. **Aggregation**: Using `MAX(CASE ...)` allows us to pivot the data such that we get the most popular female name and male name in separate columns.\n",
      "4. **Grouping**: We group by `decade` to get one row per decade.\n",
      "5. **Order**: Finally, we order the results by `decade`.\n",
      "\n",
      "This query will give you a table with each decade and the most popular male and female names based on the `quantity` of babies born with those names in that decade.\n"
     ]
    }
   ],
   "source": [
    "# example for 6 and 8, 12 and 13\n",
    "# what not to do:\n",
    "system_message = \"You are a helpful assistant.\"\n",
    "query = \"You have a table called 'my_table' with 4 columns: 'decade', 'name', 'gender', 'quantity', which represents que amount of babies born in each decade that have each name, containing also the gender information related to the name (in this case, either 'male'or 'female'). I want you to retrieve a table containing each decade and the most popular male and female names in the following columns: 'decade', 'most_popular_female_name', and 'most_popular_male_name'. This table must also be ordered by decade.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": query},\n",
    "]\n",
    "\n",
    "response = utils.get_chat_completionprompt(MODEL, messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The approach taken in the provided answer is fundamentally correct and well-structured, but there are a few areas for improvement regarding clarity, efficiency, and best practices.\n",
      "\n",
      "### Feedback:\n",
      "\n",
      "1. **Use of DISTINCT**: If there's a possibility of ties in popularity within a decade for names, using RANK() is appropriate. However, ensuring that you are aware of the implications of ties is important, and it might be beneficial to mention using `ROW_NUMBER()` instead if you only want one name regardless of ties.\n",
      "\n",
      "2. **Common Table Expressions (CTEs)**: While CTEs are clear, in some databases they can be less performant than derived tables or simple subqueries, especially if the set of data is large. If performance becomes an issue, testing with derived tables should be considered.\n",
      "\n",
      "3. **Column Selection**: When selecting columns, it’s clearer to manage them in the first place. Instead of including `quantity` in the CTEs and not using it later, it can be excluded unless necessary. \n",
      "\n",
      "4. **Cleaner Results**: The final select statement can be simplified for readability, making it clear where each name originates (i.e., from female or male CTE).\n",
      "\n",
      "### Final Optimized Query\n",
      "\n",
      "Based on the suggestions above, here's a slightly modified version of the original SQL code, maintaining the efficiency while enhancing readability and clarity:\n",
      "\n",
      "```sql\n",
      "WITH popular_female AS (\n",
      "    SELECT decade, name AS most_popular_female_name,\n",
      "           RANK() OVER (PARTITION BY decade ORDER BY quantity DESC) as rnk\n",
      "    FROM my_table\n",
      "    WHERE gender = 'female'\n",
      "),\n",
      "popular_male AS (\n",
      "    SELECT decade, name AS most_popular_male_name,\n",
      "           RANK() OVER (PARTITION BY decade ORDER BY quantity DESC) as rnk\n",
      "    FROM my_table\n",
      "    WHERE gender = 'male'\n",
      ")\n",
      "SELECT pf.decade, pf.most_popular_female_name, pm.most_popular_male_name\n",
      "FROM popular_female pf\n",
      "JOIN popular_male pm ON pf.decade = pm.decade\n",
      "WHERE pf.rnk = 1 AND pm.rnk = 1\n",
      "ORDER BY pf.decade;\n",
      "```\n",
      "\n",
      "### Summary\n",
      "\n",
      "The final SQL query remains consistent, effectively retrieving the most popular male and female names by decade while ensuring the format is neatly organized. The small tweaks introduced focus on performance, efficiency, and clarity without altering the fundamental structure or logic of the original solution. This ensures it runs optimally while maintaining easy readability, especially when revisiting the code later or sharing it with teammates.\n"
     ]
    }
   ],
   "source": [
    "# example for 6 and 8, 12 and 13\n",
    "# what to do:\n",
    "system_message = \"You are a MySQL database specialist, having graduated in Computer Science in MIT, a MsC Degree in Stanford and having worked with databases for 5 years in Facebook/Meta. You are given a query in english together with a schema and you must provide the correct MySQL query to retrieve the data as requested. Your query is at the same time very elegant and optimized to be able to run spending the least amount possible of resources. From your experience, you first write the basic idea of the query to only, after some reflection, give the final answer. If necessary, write the query in multiple steps, but always in a clear and concise way, and then the final answer.\"\n",
    "query = \"Given a table called 'my_table' with 4 columns: 'decade', 'name', 'gender', 'quantity', which represents que amount of babies born in each decade that have each name, containing also the gender information related to the name (in this case, either 'male'or 'female'). I want you to retrieve a table containing each decade and the most popular male and female names in the following columns: 'decade', 'most_popular_female_name', and 'most_popular_male_name'. This table must also be ordered by decade.\"\n",
    "second_system_message = \"The following is an answer by your colleague, a junior developer, to the same question. Please review it, providing feedback on how it could be improved. Then give a final answer.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": query},\n",
    "]\n",
    "\n",
    "first_response_response = utils.get_chat_completionprompt(MODEL, messages)\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": query},\n",
    "    {\"role\": \"system\", \"content\": second_system_message},\n",
    "    {\"role\": \"user\", \"content\": first_response_response},\n",
    "]\n",
    "final_response = utils.get_chat_completionprompt(MODEL, messages)\n",
    "print(final_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Use the practices highlighted above to create a function that has the user query as the input and outputs a final response. The task this function should solve is to balance chemical equations. Feel free to chose another task that is as difficult as this one, if you want. This exercise includes creating conversational prompts to solve the problem in multiple steps and parse the output for a final solution.\n",
    "\n",
    "**Homework:** Create multiple examples and evaluate the performance of your function, try making small changes to the prompt and chosen model to improve it.\n",
    "\n",
    "**Challenge:** Automate the process of prompting generation and performance checking so you can automatically improve your function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG (Retrieval Augmented Generation)\n",
    "\n",
    "This practice is a combination of retrieval models and generative models. The idea is to use a retrieval model to retrieve a set of documents that are relevant to the task and then use a generative model to generate the final response with the retrieved information helping the accuracy and preventing hallucination. This can be done in multiple ways, but the most common one is to use a vector database to store the documents and retrieve them using a similarity metric. The generative model can be used to generate the final response using the retrieved documents as context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First step: chunk the base text into smaller parts\n",
    "\n",
    "We usually have a very large database of texts to organize, and the best way to store that for retrieval is to chunk it into smaller parts. This can be done by paragraphs or sentences, and that depends on how much information each chunk can have and how large can it be depending not only on how well we can retrieve but also on how well the generative model can use that information, together with its context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Minneapolis[a] is a city in and the county seat of Hennepin County, Minnesota, United States. [4] With a population of 429,954, it is the state's most populous city as of the 2020 census. [7] Located in the state's center near the eastern border, it occupies both banks of the Upper Mississippi River and adjoins Saint Paul, the state capital of Minnesota.  Minneapolis, Saint Paul, and the surrounding area are collectively known as the Twin Cities, a metropolitan area with 3. 69 million residents.\",\n",
       " '[14] Minneapolis is built on an artesian aquifer on flat terrain and is known for cold, snowy winters and hot, humid summers.  Nicknamed the \"City of Lakes\",[15] Minneapolis is abundant in water, with thirteen lakes, wetlands, the Mississippi River, creeks, and waterfalls.  The city\\'s public park system is connected by the Grand Rounds National Scenic Byway. .',\n",
       " \"Dakota people originally inhabited the site of today's Minneapolis.  European colonization and settlement began north of Fort Snelling along Saint Anthony Falls—the only natural waterfall on the Mississippi River. [16] Location near the fort and the falls' power—with its potential for industrial activity—fostered the city's early growth.\",\n",
       " 'For a time in the 19th century, Minneapolis was the lumber and flour milling capital of the world, and as home to the Federal Reserve Bank of Minneapolis, it has preserved its financial clout into the 21st century.  A Minneapolis Depression-era labor strike brought about federal worker protections.  Work in Minneapolis contributed to the computing industry, and the city is the birthplace of General Mills, the Pillsbury brand, Target Corporation, and Thermo King mobile refrigeration. .',\n",
       " \"The city's major arts institutions include the Minneapolis Institute of Art, the Walker Art Center, and the Guthrie Theater.  Four professional sports teams play downtown.  Prince is survived by his favorite venue, the First Avenue nightclub.  Minneapolis is home to the University of Minnesota's main campus.  The city's public transport is provided by Metro Transit, and the international airport, serving the Twin Cities region, is located towards the south on the city limits. .\",\n",
       " 'Residents adhere to more than fifty religions.  Despite its well-regarded quality of life,[17] Minneapolis has stark disparities among its residents—arguably the most critical issue confronting the city in the 21st century. [18] Governed by a mayor-council system, Minneapolis has a political landscape dominated by the Minnesota Democratic–Farmer–Labor Party (DFL), with Jacob Frey serving as mayor since 2018. .',\n",
       " 'Two Indigenous nations inhabited the area now called Minneapolis. [19] Archaeologists have evidence that since 1000 A. D. ,[20] they were the Dakota (one half of the Sioux nation),[21] and, after the 1700s,[22] the Ojibwe (also known as Chippewa, members of the Anishinaabe nations). [23] Dakota people have different stories to explain their creation. [24] One widely accepted story says the Dakota emerged from Bdóte,[24] the confluence of the Minnesota and Mississippi rivers.',\n",
       " 'Dakota are the only inhabitants of the Minneapolis area who claimed no other land;[25] they have no traditions of having immigrated. [26] In 1680, cleric Louis Hennepin, who was probably the first European to see the Minneapolis waterfall the Dakota people call Owámniyomni, renamed it the Falls of St.  Anthony of Padua for his patron saint. [27].',\n",
       " 'In the space of sixty years, the US seized all of the Dakota land and forced them out of their homeland. [30] Purchasing most of modern-day Minneapolis, Zebulon Pike made the 1805 Treaty of St.  Peter with the Dakota.',\n",
       " '[b] Pike bought a 9-square-mile (23 km2) strip of land—coinciding with the sacred place of Dakota origin[24]—on the Mississippi south of Saint Anthony Falls,[34] with the agreement the US would build a military fort and trading post there and the Dakota would retain their usufructuary rights. [35] In 1819, the US Army built Fort Snelling[36] to direct Native American trade away from British-Canadian traders and to deter war between the Dakota and Ojibwe in northern Minnesota.',\n",
       " '[37] Under pressure from US officials[38] in a series of treaties, the Dakota ceded their land first to the east and then to the west of the Mississippi, the river that runs through Minneapolis. [39][c] Dakota leaders twice refused to sign the next treaty until they were paid for the previous one. [51] In the decades following these treaty signings, the federal US government rarely honored their terms.',\n",
       " '[52] At the beginning of the American Civil War, annuity payments owed in June 1862 to the Dakota by treaty were late, causing acute hunger among the Dakota. [53][d] Facing starvation[55] a faction of the Dakota declared war in August and killed settlers. [56] Serving without any prior military experience, US commander Henry Sibley commanded raw recruits,[57] volunteer mounted troops from Minneapolis and Saint Paul with no military experience.',\n",
       " '[58] The war went on for six weeks in the Minnesota River valley. [59] After a kangaroo court,[60][e] 38 Dakota men were hanged. [59] [f] The army force-marched 1,700 non-hostile Dakota men, women, children, and elders 150 miles (240 km) to a concentration camp at Fort Snelling. [28][77] Minneapolitans reportedly threatened more than once to attack the camp. [78] In 1863, the US \"abrogated and annulled\" all treaties with the Dakota.',\n",
       " '[79] With Governor Alexander Ramsey calling for their extermination,[80] most Dakota were exiled from Minnesota. [81].',\n",
       " \"While the Dakota were being expelled, Franklin Steele laid claim to the east bank of Saint Anthony Falls,[82] and John H.  Stevens built a home on the west bank. [83] In the Dakota language, the city's name is Bde Óta Othúŋwe ('Many Lakes Town'). [g] Residents had divergent ideas on names for their community.  Charles Hoag proposed combining the Dakota word for 'water' (mni\\u2009[h]) with the Greek word for 'city' (polis), yielding Minneapolis.\",\n",
       " \"In 1851, after a meeting of the Minnesota Territorial Legislature, leaders of east bank St.  Anthony lost their bid to move the capital from Saint Paul, but they eventually won the state university. [90] In 1856, the territorial legislature authorized Minneapolis as a town on the Mississippi's west bank. [86] Minneapolis was incorporated as a city in 1867, and in 1872, it merged with St.  Anthony. [91].\",\n",
       " \"Minneapolis originated around a source of energy: Saint Anthony Falls, the only natural waterfall on the Mississippi. [16] Each of the city's two founding industries—flour and lumber milling—developed in the 19th century nearly concurrently, and each came to prominence for about fifty years. [j] In 1884, the value of Minneapolis flour milling was the world's highest. [96] In 1899, Minneapolis outsold every other lumber market in the world.\",\n",
       " '[97] Through its expanding mill industries, Minneapolis earned the nickname \"Mill City\". [98] Due to the occupational hazards of milling, six companies manufactured artificial limbs. [99].',\n",
       " \"Disasters struck in the late 19th century: the Eastman tunnel under the river leaked in 1869; twice, fire destroyed the entire row of sawmills on the east bank;[100] an explosion of flour dust at the Washburn A mill killed eighteen people[101] and demolished about half the city's milling capacity;[102] and in 1893, fire spread from Nicollet Island to Boom Island to northeast Minneapolis, destroyed twenty blocks, and killed two people. [103].\",\n",
       " \"The lumber industry was built around forests in northern Minnesota, largely by lumbermen emigrating from Maine's depleting forests. [104][105] The region's waterways were used to transport logs well after railroads developed; the Mississippi River carried logs to St.  Louis until the early 20th century. [106] In 1871, of the thirteen mills sawing lumber in St.  Anthony, eight ran on water power, and five ran on steam power.\",\n",
       " \"[107] Auxiliary businesses on the river's west bank included woolen mills, iron works, a railroad machine shop, and mills for cotton, paper, sashes, and wood-planing. [108] Minneapolis supplied the materials for farmsteads and settlement of rapidly expanding cities on the prairies that lacked wood. [109] White pine milled in Minneapolis built Miles City, Montana; Bismarck, North Dakota; Sioux Falls, South Dakota; Omaha, Nebraska; and Wichita, Kansas.\",\n",
       " \"[110] Growing use of steam power freed lumbermen and their sawmills from dependence on the falls. [111] Lumbering's decline began around the turn of the century,[112] and sawmills in the city including the Weyerhauser mill closed by 1919. [113] After depleting Minnesota's white pine,[114] some lumbermen moved on to Douglas fir in the Pacific Northwest. [115].\",\n",
       " 'Seymour Cray and colleagues began work on the CDC 6600 (pictured) in downtown Minneapolis and completed the project in Chippewa Falls, Wisconsin, in 1963. [116].',\n",
       " 'In 1877, Cadwallader C.  Washburn co-founded Washburn-Crosby,[117] the company that became General Mills. [118][k] Washburn and partner John Crosby[119] sent Austrian civil engineer William de la Barre to Hungary where he acquired innovations through industrial espionage. [120] De la Barre calculated and managed the power at the falls and encouraged steam for auxiliary power. [121] Charles Alfred Pillsbury and the C.  A.',\n",
       " 'Pillsbury Company across the river hired Washburn-Crosby employees and began using the new methods. [120] The hard red spring wheat grown in Minnesota became valuable, and Minnesota \"patent\" flour was recognized at the time as the best bread flour in the world. [120] In 1900, fourteen percent of America\\'s grain was milled in Minneapolis[120] and about one third of that was shipped overseas. [122] Overall production peaked at 18. 5 million barrels in 1916.',\n",
       " \"[123] Decades of soil exhaustion, stem rust, and changes in freight tariffs combined to quash the city's flour industry. [124] In the 1920s, Washburn-Crosby and Pillsbury developed new milling centers in Buffalo, New York, and Kansas City, Missouri, while maintaining their headquarters in Minneapolis. [125] The falls became a national historic district,[126] and the upper St.  Anthony lock and dam is permanently closed. [127].\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chunking example\n",
    "text = \"\"\"Minneapolis\n",
    "\n",
    "Article\n",
    "Talk\n",
    "Read\n",
    "View source\n",
    "View history\n",
    "\n",
    "Tools\n",
    "Appearance hide\n",
    "Text\n",
    "\n",
    "Small\n",
    "\n",
    "Standard\n",
    "\n",
    "Large\n",
    "Width\n",
    "\n",
    "Standard\n",
    "\n",
    "Wide\n",
    "Color (beta)\n",
    "\n",
    "Automatic\n",
    "\n",
    "Light\n",
    "\n",
    "Dark\n",
    "Coordinates: 44°58′55″N 93°16′09″W\n",
    "Featured article\n",
    "Page extended-protected\n",
    "From Wikipedia, the free encyclopedia\n",
    "This article is about the city in Minnesota. For other uses, see Minneapolis (disambiguation).\n",
    "For the Twin Cities region, see Minneapolis–Saint Paul.\n",
    "\"Mpls.\" redirects here. For other topics, see mpls (disambiguation).\n",
    "Minneapolis\n",
    "City\n",
    "Downtown Minneapolis (from the Mississippi River)\n",
    "Downtown Minneapolis (from the Mississippi River)\n",
    "Bde Maka Ska\n",
    "Bde Maka Ska\n",
    "Mill City Museum\n",
    "Mill City Museum\n",
    "First Avenue, a nightclub\n",
    "First Avenue\n",
    "Minnehaha Falls\n",
    "Minnehaha Falls\n",
    "Flag of Minneapolis\n",
    "Flag\n",
    "Official seal of Minneapolis\n",
    "Seal\n",
    "Official logo of Minneapolis\n",
    "Logo\n",
    "Etymology: Dakota mni 'water' with Greek polis 'city'\n",
    "Nicknames: \"City of Lakes\",[1] \"Mill City\",[1] \"Twin Cities\"[2] (with Saint Paul), \"Mini Apple\"[1]\n",
    "Motto: En Avant (French: 'Forward')[3]\n",
    "MapWikimedia | © OpenStreetMap\n",
    "Show Minneapolis\n",
    "Show Hennepin County\n",
    "Show Minnesota\n",
    "Show the United States\n",
    "Show all\n",
    "Coordinates: 44°58′55″N 93°16′09″W[4]\n",
    "Country\tUnited States\n",
    "State\tMinnesota\n",
    "County\tHennepin\n",
    "Incorporated\t1867\n",
    "Founded by\tFranklin Steele and John H. Stevens\n",
    "Government\n",
    " • Type\tMayor–council (strong mayor)[5]\n",
    " • Body\tMinneapolis City Council\n",
    " • Mayor\tJacob Frey (DFL)\n",
    "Area[6]\n",
    " • City\n",
    "57.51 sq mi (148.94 km2)\n",
    " • Land\t54.00 sq mi (139.86 km2)\n",
    " • Water\t3.51 sq mi (9.08 km2)\n",
    "Elevation[4]\t830 ft (250 m)\n",
    "Population (2020)[7]\n",
    " • City\n",
    "429,954\n",
    " • Estimate (2023)[8]\n",
    "425,115\n",
    " • Rank\t\n",
    "46th (US)\n",
    "1st (Minnesota)\n",
    " • Density\t7,962.11/sq mi (3,074.21/km2)\n",
    " • Urban[9]\t2,914,866\n",
    " • Urban density\t2,872.4/sq mi (1,109/km2)\n",
    " • Metro[10]\t3,693,729\n",
    "Demonym\tMinneapolitan\n",
    "GDP[11]\n",
    " • MSA\t$323.9 billion (2022) ($337 billion in 2023)[12]\n",
    "Time zone\tUTC–6 (Central)\n",
    " • Summer (DST)\tUTC–5 (CDT)\n",
    "ZIP Codes\t\n",
    "55401-55419, 55423, 55429-55430, 55450, 55454-55455, 55484-55488\n",
    "Area code\t612\n",
    "FIPS code\t27-43000[4]\n",
    "GNIS ID\t655030[4]\n",
    "Website\tminneapolismn.gov\n",
    "Minneapolis[a] is a city in and the county seat of Hennepin County, Minnesota, United States.[4] With a population of 429,954, it is the state's most populous city as of the 2020 census.[7] Located in the state's center near the eastern border, it occupies both banks of the Upper Mississippi River and adjoins Saint Paul, the state capital of Minnesota. Minneapolis, Saint Paul, and the surrounding area are collectively known as the Twin Cities, a metropolitan area with 3.69 million residents.[14] Minneapolis is built on an artesian aquifer on flat terrain and is known for cold, snowy winters and hot, humid summers. Nicknamed the \"City of Lakes\",[15] Minneapolis is abundant in water, with thirteen lakes, wetlands, the Mississippi River, creeks, and waterfalls. The city's public park system is connected by the Grand Rounds National Scenic Byway.\n",
    "\n",
    "Dakota people originally inhabited the site of today's Minneapolis. European colonization and settlement began north of Fort Snelling along Saint Anthony Falls—the only natural waterfall on the Mississippi River.[16] Location near the fort and the falls' power—with its potential for industrial activity—fostered the city's early growth. For a time in the 19th century, Minneapolis was the lumber and flour milling capital of the world, and as home to the Federal Reserve Bank of Minneapolis, it has preserved its financial clout into the 21st century. A Minneapolis Depression-era labor strike brought about federal worker protections. Work in Minneapolis contributed to the computing industry, and the city is the birthplace of General Mills, the Pillsbury brand, Target Corporation, and Thermo King mobile refrigeration.\n",
    "\n",
    "The city's major arts institutions include the Minneapolis Institute of Art, the Walker Art Center, and the Guthrie Theater. Four professional sports teams play downtown. Prince is survived by his favorite venue, the First Avenue nightclub. Minneapolis is home to the University of Minnesota's main campus. The city's public transport is provided by Metro Transit, and the international airport, serving the Twin Cities region, is located towards the south on the city limits.\n",
    "\n",
    "Residents adhere to more than fifty religions. Despite its well-regarded quality of life,[17] Minneapolis has stark disparities among its residents—arguably the most critical issue confronting the city in the 21st century.[18] Governed by a mayor-council system, Minneapolis has a political landscape dominated by the Minnesota Democratic–Farmer–Labor Party (DFL), with Jacob Frey serving as mayor since 2018.\n",
    "\n",
    "History\n",
    "Main article: History of Minneapolis\n",
    "Dakota homeland\n",
    "Further information: Dakota people, Ojibwe, Bdóte, and US–Dakota War of 1862\n",
    "Two Indigenous nations inhabited the area now called Minneapolis.[19] Archaeologists have evidence that since 1000 A.D.,[20] they were the Dakota (one half of the Sioux nation),[21] and, after the 1700s,[22] the Ojibwe (also known as Chippewa, members of the Anishinaabe nations).[23] Dakota people have different stories to explain their creation.[24] One widely accepted story says the Dakota emerged from Bdóte,[24] the confluence of the Minnesota and Mississippi rivers. Dakota are the only inhabitants of the Minneapolis area who claimed no other land;[25] they have no traditions of having immigrated.[26] In 1680, cleric Louis Hennepin, who was probably the first European to see the Minneapolis waterfall the Dakota people call Owámniyomni, renamed it the Falls of St. Anthony of Padua for his patron saint.[27]\n",
    "\n",
    "Island covered with hundreds of teepees\n",
    "Dakota non-combatants living in a concentration camp at Fort Snelling during the winter of 1862[28][29]\n",
    "In the space of sixty years, the US seized all of the Dakota land and forced them out of their homeland.[30] Purchasing most of modern-day Minneapolis, Zebulon Pike made the 1805 Treaty of St. Peter with the Dakota.[b] Pike bought a 9-square-mile (23 km2) strip of land—coinciding with the sacred place of Dakota origin[24]—on the Mississippi south of Saint Anthony Falls,[34] with the agreement the US would build a military fort and trading post there and the Dakota would retain their usufructuary rights.[35] In 1819, the US Army built Fort Snelling[36] to direct Native American trade away from British-Canadian traders and to deter war between the Dakota and Ojibwe in northern Minnesota.[37] Under pressure from US officials[38] in a series of treaties, the Dakota ceded their land first to the east and then to the west of the Mississippi, the river that runs through Minneapolis.[39][c] Dakota leaders twice refused to sign the next treaty until they were paid for the previous one.[51] In the decades following these treaty signings, the federal US government rarely honored their terms.[52] At the beginning of the American Civil War, annuity payments owed in June 1862 to the Dakota by treaty were late, causing acute hunger among the Dakota.[53][d] Facing starvation[55] a faction of the Dakota declared war in August and killed settlers.[56] Serving without any prior military experience, US commander Henry Sibley commanded raw recruits,[57] volunteer mounted troops from Minneapolis and Saint Paul with no military experience.[58] The war went on for six weeks in the Minnesota River valley.[59] After a kangaroo court,[60][e] 38 Dakota men were hanged.[59] [f] The army force-marched 1,700 non-hostile Dakota men, women, children, and elders 150 miles (240 km) to a concentration camp at Fort Snelling.[28][77] Minneapolitans reportedly threatened more than once to attack the camp.[78] In 1863, the US \"abrogated and annulled\" all treaties with the Dakota.[79] With Governor Alexander Ramsey calling for their extermination,[80] most Dakota were exiled from Minnesota.[81]\n",
    "\n",
    "While the Dakota were being expelled, Franklin Steele laid claim to the east bank of Saint Anthony Falls,[82] and John H. Stevens built a home on the west bank.[83] In the Dakota language, the city's name is Bde Óta Othúŋwe ('Many Lakes Town').[g] Residents had divergent ideas on names for their community. Charles Hoag proposed combining the Dakota word for 'water' (mni [h]) with the Greek word for 'city' (polis), yielding Minneapolis. In 1851, after a meeting of the Minnesota Territorial Legislature, leaders of east bank St. Anthony lost their bid to move the capital from Saint Paul, but they eventually won the state university.[90] In 1856, the territorial legislature authorized Minneapolis as a town on the Mississippi's west bank.[86] Minneapolis was incorporated as a city in 1867, and in 1872, it merged with St. Anthony.[91]\n",
    "\n",
    "Industries develop\n",
    "Waterfall surrounded by mills and scaffolding\n",
    "Saint Anthony Falls c. 1850s\n",
    "Two men loaded flour\n",
    "Loading flour, Pillsbury, 1939\n",
    "Minneapolis originated around a source of energy: Saint Anthony Falls, the only natural waterfall on the Mississippi.[16] Each of the city's two founding industries—flour and lumber milling—developed in the 19th century nearly concurrently, and each came to prominence for about fifty years.[j] In 1884, the value of Minneapolis flour milling was the world's highest.[96] In 1899, Minneapolis outsold every other lumber market in the world.[97] Through its expanding mill industries, Minneapolis earned the nickname \"Mill City\".[98] Due to the occupational hazards of milling, six companies manufactured artificial limbs.[99]\n",
    "\n",
    "Disasters struck in the late 19th century: the Eastman tunnel under the river leaked in 1869; twice, fire destroyed the entire row of sawmills on the east bank;[100] an explosion of flour dust at the Washburn A mill killed eighteen people[101] and demolished about half the city's milling capacity;[102] and in 1893, fire spread from Nicollet Island to Boom Island to northeast Minneapolis, destroyed twenty blocks, and killed two people.[103]\n",
    "\n",
    "The lumber industry was built around forests in northern Minnesota, largely by lumbermen emigrating from Maine's depleting forests.[104][105] The region's waterways were used to transport logs well after railroads developed; the Mississippi River carried logs to St. Louis until the early 20th century.[106] In 1871, of the thirteen mills sawing lumber in St. Anthony, eight ran on water power, and five ran on steam power.[107] Auxiliary businesses on the river's west bank included woolen mills, iron works, a railroad machine shop, and mills for cotton, paper, sashes, and wood-planing.[108] Minneapolis supplied the materials for farmsteads and settlement of rapidly expanding cities on the prairies that lacked wood.[109] White pine milled in Minneapolis built Miles City, Montana; Bismarck, North Dakota; Sioux Falls, South Dakota; Omaha, Nebraska; and Wichita, Kansas.[110] Growing use of steam power freed lumbermen and their sawmills from dependence on the falls.[111] Lumbering's decline began around the turn of the century,[112] and sawmills in the city including the Weyerhauser mill closed by 1919.[113] After depleting Minnesota's white pine,[114] some lumbermen moved on to Douglas fir in the Pacific Northwest.[115]\n",
    "\n",
    "Large computer terminal\n",
    "Seymour Cray and colleagues began work on the CDC 6600 (pictured) in downtown Minneapolis and completed the project in Chippewa Falls, Wisconsin, in 1963.[116]\n",
    "In 1877, Cadwallader C. Washburn co-founded Washburn-Crosby,[117] the company that became General Mills.[118][k] Washburn and partner John Crosby[119] sent Austrian civil engineer William de la Barre to Hungary where he acquired innovations through industrial espionage.[120] De la Barre calculated and managed the power at the falls and encouraged steam for auxiliary power.[121] Charles Alfred Pillsbury and the C. A. Pillsbury Company across the river hired Washburn-Crosby employees and began using the new methods.[120] The hard red spring wheat grown in Minnesota became valuable, and Minnesota \"patent\" flour was recognized at the time as the best bread flour in the world.[120] In 1900, fourteen percent of America's grain was milled in Minneapolis[120] and about one third of that was shipped overseas.[122] Overall production peaked at 18.5 million barrels in 1916.[123] Decades of soil exhaustion, stem rust, and changes in freight tariffs combined to quash the city's flour industry.[124] In the 1920s, Washburn-Crosby and Pillsbury developed new milling centers in Buffalo, New York, and Kansas City, Missouri, while maintaining their headquarters in Minneapolis.[125] The falls became a national historic district,[126] and the upper St. Anthony lock and dam is permanently closed.[127]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "chunks = utils.document_based_chunking(text)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second step: vectorize the chunks\n",
    "\n",
    "We use embedding models to convert the chunks into vectors, so we can use a similarity metric to retrieve them. The most common models are Sentence Transformers (based in standard Transformer Encoder models like BERT, RoBERTa, etc.) and API provided models like OpenAI's `text-embedding-3` or `text-embedding-ada-002`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.016855744644999504, -0.008043641224503517, 0.055278778076171875, 0.037616778165102005, -0.08470704406499863, -0.030637646093964577, -0.014890500344336033, -0.01698172092437744, 0.004176142625510693, -0.01457555778324604, -0.00018916257249657065, 0.0036690847482532263, 0.0057603055611252785, -0.028848769143223763, -0.02597649022936821, 0.018228894099593163, 0.018002135679125786, -0.022134186699986458, -0.007004329934716225, 0.03696169704198837, 0.025220626965165138, 0.05225532501935959, -0.03338394686579704, 0.0009802597342059016, 0.02019413933157921, 0.02635442093014717, -0.03681052476167679, 0.040136322379112244, -0.03701208904385567, -0.012056014500558376, -0.03031010366976261, -0.008969573304057121, -0.004576120525598526, 0.012465439736843109, 0.04812327399849892, 0.024389177560806274, -0.0010306506883352995, -0.017498226836323738, -0.036734938621520996, -0.029982563108205795, 0.013819694519042969, 0.013089027255773544, 0.012188290245831013, -0.002258140593767166, -0.03623102977871895, 0.02733704447746277, -0.013580338098108768, 0.023393958806991577, 0.01650300808250904, 0.012408750131726265, -0.005826443433761597, 0.003968280740082264, -0.039934758096933365, -0.008339688181877136, -0.02887396514415741, -0.007061020005494356, 0.0197910126298666, -0.00942309107631445, 0.0004279286658857018, -0.024666327983140945, 0.0279921256005764, -0.012137899175286293, 0.01143872644752264, 0.04240391030907631, 0.021365726366639137, 0.039531633257865906, -0.007917664013803005, -0.014915695413947105, -0.006960237864404917, 0.0502396896481514, 0.003539958270266652, 0.021315336227416992, 0.01424801629036665, -0.023620717227458954, -0.005086327902972698, -0.02751341089606285, -0.04651076719164848, -0.016150271520018578, -0.0685315728187561, 0.050441253930330276, 0.01342916488647461, 0.013265394605696201, -0.06525617092847824, -0.018342275172472, -0.0123079689219594, 0.02723626233637333, -0.02210899069905281, -0.010909621603786945, 0.01270479615777731, 0.010367920622229576, 0.03350992500782013, 0.040715817362070084, 0.03464371711015701, -0.013933073729276657, 0.024842696264386177, 0.027261456474661827, 0.030108541250228882, 0.015356616117060184, 0.017800573259592056, 0.024552948772907257, -0.022134186699986458, -0.0492318756878376, 0.036835722625255585, 0.013844889588654041, 0.04847601056098938, -0.05457330495119095, 0.018266689032316208, 0.011848151683807373, 0.05230571702122688, 0.03633181378245354, -0.0472918264567852, -0.028521228581666946, 0.045906078070402145, 0.020496483892202377, -0.04169844090938568, 0.03618064150214195, -0.03875057399272919, -0.00463281013071537, 0.019728023558855057, -0.03245171532034874, -0.04683830961585045, 0.03328316658735275, 0.001525898347608745, 0.006777571048587561, 0.026102468371391296, 0.0120623130351305, 0.02877318300306797, 0.059612393379211426, -0.034240592271089554, 0.003313199384137988, 0.0013015015283599496, -0.01642742194235325, -0.004280074033886194, -0.010330126620829105, 0.008175917901098728, -0.036558572202920914, -0.03232574090361595, 0.04943343624472618, 0.0003647432313300669, 0.0038517515640705824, 0.02480490319430828, 0.002928968984633684, 0.06958978623151779, 0.003867498831823468, 0.034341372549533844, -0.013605533167719841, 0.004059614147990942, 0.04807288572192192, 0.034618522971868515, -0.049760978668928146, 0.02471671812236309, 0.03975839167833328, 0.0178509633988142, -0.02741263061761856, -0.050743598490953445, -0.01613767445087433, 0.007375962566584349, -0.014336200430989265, 0.016377029940485954, -0.014147235080599785, -0.041572462767362595, 0.0036186939105391502, 0.04696428403258324, -0.046888697892427444, -0.0188209880143404, -0.00219515198841691, -0.04769495129585266, -0.04845081642270088, -0.034341372549533844, -0.04121972620487213, 0.031443897634744644, 0.01573454774916172, 0.015255833975970745, 0.0328296460211277, -0.0007971992017701268, -0.04696428403258324, -0.017800573259592056, 0.001993588637560606, 0.011564703658223152, -0.024401776492595673, 0.0127866817638278, -0.006859456188976765, 0.02993217296898365, -0.04527619108557701, -0.0385994017124176, 0.012358359061181545, -0.0415220707654953, 0.018606826663017273, 0.008182216435670853, 0.016754962503910065, 0.037037283182144165, -0.00873651634901762, 0.0125473253428936, -0.02270108461380005, 0.007073617540299892, -0.037515997886657715, 0.005675271153450012, -0.032577693462371826, 0.0018439906416460872, -0.011545807123184204, 0.01394567172974348, 0.005555592942982912, -0.013807096518576145, 0.016074685379862785, -0.016956524923443794, 0.03746560588479042, 0.002039255341514945, -0.0569920651614666, 0.020420897752046585, -0.06812844425439835, 0.040992967784404755, -0.03880096599459648, -0.001933749415911734, 0.017737584188580513, 0.002928968984633684, 0.008862493559718132, -0.03733963146805763, -0.04091738164424896, 0.03776795417070389, -0.008711320348083973, 0.01893436722457409, 0.003373038489371538, -0.02627883478999138, 0.022234968841075897, -0.013844889588654041, -0.00854125153273344, -0.04847601056098938, -0.0037226250860840082, -0.05240649729967117, 0.022260164842009544, 0.036054663360118866, -0.025031661614775658, -0.024187615141272545, -0.02741263061761856, -0.017913952469825745, -0.021214554086327553, 0.01167808286845684, 0.0010054551530629396, -0.024590741842985153, 0.06359326839447021, -0.017913952469825745, -0.038674987852573395, 0.028647206723690033, 0.0694890022277832, -0.0094104940071702, 0.010695461183786392, -0.0074641467072069645, 0.052759233862161636, -0.02008076012134552, 0.04663674533367157, -0.021491702646017075, 0.015570776537060738, 0.02290264703333378, 0.0067838700488209724, -0.005111523438245058, -0.009901804849505424, 0.031040772795677185, 0.004878465551882982, -0.0015195994637906551, 0.028218884021043777, -0.01612507551908493, 0.027840951457619667, -0.016087282449007034, 0.02549777738749981, -0.038674987852573395, -0.019828805699944496, 0.06132568046450615, 0.00371632631868124, 0.03975839167833328, 0.041950393468141556, -0.02481750026345253, -0.029503850266337395, -0.021655473858118057, -0.05618581175804138, -0.03338394686579704, -0.031721048057079315, -0.007394859101623297, -0.042731452733278275, 0.017246272414922714, 0.04006073623895645, 0.03408941999077797, 0.03466891497373581, 0.0053256843239068985, 0.04530138522386551, -0.002185703720897436, 0.012106404639780521, 0.001940048299729824, 0.011829255148768425, -0.01970282755792141, -0.014638545922935009, -0.02723626233637333, -0.042529888451099396, -0.020042965188622475, -0.03293043002486229, -0.02113896794617176, 0.01486530527472496, -0.05815105512738228, 0.012295370921492577, 0.00685315765440464, -0.02761419303715229, -0.005889432039111853, 0.020899610593914986, 0.02645520307123661, -0.020332712680101395, -0.04711545631289482, 0.03232574090361595, 0.0036911307834088802, 0.03633181378245354, 0.0636940523982048, 0.047442998737096786, -0.02241133712232113, -0.011867048218846321, 0.05608503147959709, 0.0236459132283926, -0.027765365317463875, -0.016918731853365898, -0.0026817386969923973, -0.005599684547632933, -0.01201822143048048, 0.0003277374489698559, -0.03214937075972557, -0.049685392528772354, 0.03522321209311485, 0.009246723726391792, -0.007936561480164528, 0.022663291543722153, 0.010103368200361729, -0.03950643539428711, -0.02781575731933117, 0.013454360887408257, -0.014764523133635521, 0.00559653528034687, -0.039934758096933365, 0.02723626233637333, -0.039733197540044785, 0.008837297558784485, -0.028370056301355362, -0.0071177096106112, -0.0053067877888679504, 0.06026747450232506, -0.061930373311042786, 0.018858781084418297, -0.03532399609684944, -0.049383047968149185, 0.022260164842009544, -0.011665484867990017, -0.043310947716236115, 0.05104594677686691, 0.010449805296957493, -0.043588098138570786, -0.041849613189697266, -0.006752375978976488, 0.057999882847070694, 0.001722737681120634, -0.04787132143974304, 0.022953039035201073, -0.007394859101623297, -0.06722141057252884, 0.027362238615751266, 0.037037283182144165, -0.026203248649835587, -0.05427096039056778, -0.010223046876490116, -0.008188514970242977, -0.044469937682151794, -0.013290590606629848, -0.04525099694728851, -0.016062088310718536, -0.013353578746318817, 0.030662840232253075, 0.002199876122176647, -0.022801866754889488, -0.07095033675432205, 0.04525099694728851, -0.018115514889359474, 0.10682862997055054, 0.028143297880887985, 0.012723692692816257, 0.018883977085351944, 0.008484561927616596, 0.014663740992546082, -0.002466002944856882, 0.0415220707654953, -0.03408941999077797, 0.020118553191423416, 0.033257968723773956, -0.0026911869645118713, -0.029277091845870018, 0.00571936322376132, -0.01622585766017437, 0.027211066335439682, -0.021655473858118057, 0.020269725471735, 0.054220568388700485, -0.02377188950777054, 0.010015184059739113, 0.004314717836678028, -0.018594229593873024, 0.01913592964410782, 0.048526402562856674, -0.03227534890174866, 0.01892177015542984, -0.007136606145650148, -0.03756638988852501, 0.053061578422784805, 0.035374388098716736, -0.017724987119436264, 0.004261177498847246, 0.004166694357991219, -0.0492822639644146, -0.014953488484025002, -0.07130306959152222, -0.00056689721532166, -0.04698948189616203, 0.037793148308992386, 0.017246272414922714, 0.008308193646371365, -0.007659411523491144, 0.012604014948010445, 0.043210167437791824, 0.01960204541683197, 0.03602946922183037, -0.081179678440094, -0.022159382700920105, -0.05870535597205162, 0.031166749075055122, 0.01727146841585636, 0.013517349027097225, 0.03585309907793999, 0.012440244667232037, -0.01307642925530672, 0.015885720029473305, 0.014953488484025002, -0.023154601454734802, -0.0120623130351305, -0.036155443638563156, -0.011426128447055817, 0.03232574090361595, -0.002163657685741782, -0.04703987017273903, 0.0049446034245193005, -0.051701027899980545, -0.04132050648331642, -0.007854675874114037, 0.03436657041311264, -0.03350992500782013, -0.0025494627188891172, 0.022083796560764313, -0.020055564120411873, 0.004743040073662996, 0.01853124052286148, -0.02076103538274765, 0.05714324116706848, -0.02433878742158413, -0.0016534501919522882, -0.0357523187994957, -0.03990956395864487, 0.03975839167833328, 0.021290140226483345, 0.001165288733318448, 0.020987795665860176, -0.028042515739798546, -0.0034612223971635103, 0.010330126620829105, -0.06399639695882797, -0.03449254482984543, 0.022272761911153793, -0.008194814436137676, 0.002779371105134487, -0.06349249184131622, 0.05477486923336983, 0.00032242279849015176, -0.03051166795194149, 0.0142354192212224, 0.0036155444104224443, -0.002193577354773879, -0.008068837225437164, -0.07009369134902954, 0.07251245528459549, -0.002725830767303705, 0.020811427384614944, -0.01452516671270132, -0.03534919023513794, 0.04963500052690506, 0.01235206052660942, -0.04653596132993698, -0.015344018116593361, 0.0656089037656784, -0.06747336685657501, 0.001217254321090877, -0.03907811641693115, 0.002034531207755208, -0.04557853564620018, -0.013038636185228825, 0.031922612339258194, -0.033837463706731796, -0.035979077219963074, 0.04336133971810341, 0.011085989885032177, 0.03340914100408554, -0.01458815485239029, -0.025825317949056625, -0.0502396896481514, 0.05205376073718071, 0.021075978875160217, 0.04401642084121704, 0.0006129576358944178, -0.0082011129707098, 0.041295312345027924, -0.017309261485934258, -0.03638220205903053, 0.02887396514415741, -0.020924806594848633, -0.01371891237795353, 0.0011629266664385796, 0.004708396270871162, -0.007571227382868528, 0.029377873986959457, -0.012509532272815704, -0.021302737295627594, 0.01071435771882534, 0.03580270707607269, 0.018014734610915184, -0.021264944225549698, 0.057495974004268646, 0.012459141202270985, -0.019639838486909866, 0.034895673394203186, 0.0036659352481365204, 0.024779707193374634, 0.024175016209483147, -0.0048816148191690445, -0.04260547459125519, 0.013857487589120865, 0.021579887717962265, 0.015331420116126537, 0.03885135427117348, 0.002741578035056591, 0.055177994072437286, 0.002897474681958556, 0.04950902238488197, 0.011816657148301601, -0.02548517845571041, -0.056740112602710724, 0.005224902648478746, -0.040892187505960464, -0.019337493926286697, -0.012131600640714169, -0.04179922118782997, 0.008005848154425621, -0.01583532802760601, 0.0414968766272068, -0.026959111914038658, 0.045906078070402145, 0.012912658974528313, -0.016918731853365898, 0.017044709995388985, 0.009964792989194393, 0.029982563108205795, 0.015180247835814953, -0.03222495689988136, 0.015608569607138634, 0.0009755356004461646, 0.015810133889317513, -0.028445642441511154, 0.011029300279915333, -0.002832911442965269, 0.0284708384424448, -0.019249310716986656, -0.027639389038085938, 0.011079691350460052, 0.007640514522790909, 0.006809065584093332, -0.024603338912129402, 0.007054721005260944, 0.032098978757858276, 0.006270512938499451, 0.005007592029869556, -0.01651560515165329, -0.031443897634744644, 0.022764071822166443, 0.021504301577806473, 0.00022951464052312076, -0.0027809457387775183, -0.004906810354441404, -0.00022774309036321938, -0.03784354031085968, 0.01796434260904789, 0.02935267798602581, 0.0071177096106112, -0.016490411013364792, -0.018128113821148872, -0.012333163991570473, -0.024275798350572586, 0.011501714587211609, -0.03081401251256466, -0.0011660760501399636, 0.0012511106906458735, -0.014840109273791313, 0.02607727237045765, -0.027362238615751266, -0.037213653326034546, 0.03633181378245354, -0.02499386854469776, 0.0385994017124176, -0.021201955154538155, -0.008289297111332417, -0.026782743632793427, 0.022423934191465378, -0.014197626151144505, 0.03948124125599861, -0.00622642133384943, 0.008723918348550797, 0.024653730913996696, -0.0059776161797344685, -0.0068027665838599205, 0.005495753604918718, 0.0049005113542079926, 0.022814463824033737, 0.013870085589587688, -0.0006365783629007638, 0.008503458462655544, 0.02404903993010521, 0.010689161717891693, -0.012799279764294624, 0.02299083210527897, 0.03061245009303093, -0.022373544052243233, -0.02471671812236309, -0.0038517515640705824, -0.001878634444437921, 0.008339688181877136, 0.06036825478076935, -0.002398290205746889, -0.011369438841938972, -0.017913952469825745, -0.0037824641913175583, -0.0024234855081886053, -0.012314267456531525, -0.0047272928059101105, -0.012339462526142597, -0.012843371368944645, -0.009486080147325993, 0.03051166795194149, 0.026606375351548195, -0.02829447016119957, -0.006846858654171228, 0.006188627798110247, -0.012660704553127289, -0.028924355283379555, 0.04101816192269325, -0.028319666162133217, -0.018014734610915184, 0.03640739992260933, -0.003612395143136382, -0.011312749236822128, 0.017510825768113136, -0.014638545922935009, 0.0005082390853203833, 0.012326865456998348, -0.03638220205903053, -0.012673302553594112, 0.032098978757858276, -0.007501939777284861, -0.01235206052660942, 0.024389177560806274, 0.026304030790925026, -0.028546424582600594, 0.020156346261501312, 0.021214554086327553, 0.03822147101163864, -0.0002867948787752539, 0.011602496728301048, 0.018216297030448914, 0.017347054556012154, 0.021756255999207497, 0.009498678147792816, -0.023293176665902138, 0.020496483892202377, -0.017132893204689026, 0.03081401251256466, 0.004613913595676422, -0.003009279491379857, -0.014991281554102898, 0.002873854013159871, -0.00010147066495846957, 0.03880096599459648, 0.04691389575600624, -0.04159765690565109, -0.05099555477499962, 0.020320115610957146, -0.006557111162692308, 0.06696946173906326, -0.007023226469755173, 0.016452617943286896, -0.014764523133635521, -0.018757998943328857, -0.024565545842051506, 0.015558179467916489, 0.0076279169879853725, -0.04409200698137283, 0.006352398078888655, -0.03638220205903053, -0.02675754949450493, -0.008560148067772388, 0.010128563269972801, 0.02335616573691368, -0.016389628872275352, -0.0064500304870307446, 0.014789718203246593, 0.0037793146912008524, -0.031065966933965683, 0.024200212210416794, 0.009788425639271736, 0.007955458015203476, -0.0002405376435490325, -0.029856586828827858, 0.009404194541275501, -0.020937403663992882, 0.0501893013715744, 0.023897867649793625, 0.00610359339043498, 0.016377029940485954, 0.031897418200969696, 0.0036690847482532263, 0.018279286101460457, 0.013870085589587688, 0.028521228581666946, 0.028420446440577507, -0.01029233355075121, -0.022423934191465378, -0.012792980298399925, -0.020685449242591858, 0.025648949667811394, 0.012830773368477821, -0.0284960325807333, 0.006557111162692308, 0.034618522971868515, -0.02809290587902069, -0.002579382387921214, -0.023142004385590553, 0.013252797536551952, 0.02761419303715229, -0.011445024982094765, 0.008434170857071877, 0.002502221381291747, -0.034996453672647476, 0.02270108461380005, -0.021126369014382362, 0.0019416229333728552, 0.022171979770064354, 0.009926999919116497, 0.041849613189697266, 0.02154209464788437, 0.009586862288415432, 0.02181924320757389, -0.0006066587520763278, -0.001543220249004662, -0.008131825365126133, -0.026404812932014465, 0.008579044602811337, 0.048803552985191345, -0.015331420116126537, 0.0098955063149333, 0.013265394605696201, -0.03290523588657379, -0.014109442010521889, 0.0062957084737718105, -0.013958269730210304, -0.002593554789200425, 0.006468927022069693, -0.024691523984074593, 0.01307642925530672, 0.018984757363796234, -0.002475451212376356, 0.0019920137710869312, 0.005999662447720766, -0.044873062521219254, -0.021869635209441185, 0.020105954259634018, -0.00453517772257328, 0.02615285851061344, 0.01990439184010029, 0.001842416008003056, 0.015243235975503922, -0.0042832233011722565, 0.003530510002747178, 0.008786906488239765, 0.03436657041311264, 0.00617603026330471, -0.023734096437692642, 0.07014407962560654, 0.008692423813045025, 0.032199762761592865, -0.014714132063090801, -0.013643326237797737, 0.00487531628459692, -0.06132568046450615, 0.02125234715640545, -0.03544997423887253, -0.007539732847362757, 0.026052076369524002, 0.010443506762385368, 0.02675754949450493, 0.00888139009475708, 0.02212158963084221, -0.02289004996418953, -0.0017967491876333952, 0.04232832416892052, -0.024502556771039963, -0.04741780459880829, -0.01632663980126381, -0.0023730946704745293, -0.008144423365592957, 0.019665034487843513, -0.04575490579009056, 0.018002135679125786, -0.03600427135825157, -0.0004224171570967883, -0.015281029045581818, -0.030385691672563553, -0.012364658527076244, -0.049181483685970306, 0.004714695271104574, -0.039456047117710114, -0.05046645179390907, 0.00113379443064332, 0.02038310468196869, -0.034416958689689636, -0.023091614246368408, -0.0037698664236813784, 0.02857162058353424, 0.01679275557398796, -0.006185478530824184, -0.0010101794032379985, -0.00533198332414031, 0.04197559133172035, -0.014185028150677681, -0.009208930656313896, -0.020257126539945602, -0.00326910731382668, 0.014613350853323936, -0.032880038022994995, -0.0060595013201236725, -0.007974354550242424, -0.026631571352481842, 0.018984757363796234, 0.005807546898722649, -0.04981137067079544, -0.046006858348846436, 0.008031044155359268, -0.020622460171580315, 0.017687194049358368, -0.010752150788903236, 0.022449130192399025, -0.0030124287586659193, 0.037037283182144165, 0.004843821749091148, -0.007243686821311712, 0.0010086046531796455, 0.00863573420792818, -0.023998649790883064, 0.001958944834768772, 0.07750114798545837, 0.014852707274258137, -0.018090320751070976, -0.007760193198919296, -0.02163027785718441, 0.00034702770062722266, 0.0030675437301397324, 0.022562509402632713, -0.004579269792884588, -0.00460761459544301, 0.006456329487264156, 0.002380968304350972, -0.006922444794327021, -0.0108592314645648, 0.0031982450745999813, 0.02733704447746277, -0.04169844090938568, -0.03396344184875488, 0.03678533062338829, 0.008629435673356056, -0.013240199536085129, -0.00714290514588356, -0.047165848314762115, -0.0217436570674181, -0.030562058091163635, 0.0018707608105614781, -0.03817107900977135, 0.008131825365126133, -0.02877318300306797, -0.022449130192399025, -0.06460108608007431, -0.037314433604478836, 0.017926549538969994, -0.0038202572613954544, 0.010922219604253769, -0.015797534957528114, 0.00020353184663690627, 0.03754119202494621, 0.0001926072727655992, -0.01593611016869545, -0.007615319453179836, -0.03628142178058624, -0.01612507551908493, 0.023494740948081017, 0.0318470261991024, -0.019740620627999306, 0.0113379443064332, 0.03051166795194149, 0.0060059609822928905, -0.00039879645919427276, -0.013681119307875633, -0.017447836697101593, 0.00291952071711421, -0.00100151845254004, -0.00015540463209617883, -0.017032111063599586, 0.0019164275145158172, 0.04121972620487213, -0.00300455535762012, 0.00016416398284491152, -0.007073617540299892, 0.026304030790925026, 0.020609863102436066, -0.06344209611415863, -0.006859456188976765, 0.0023589222691953182, 0.008988470770418644, 0.004106855485588312, -0.002004611538723111, -0.01641482301056385, 0.03804510086774826, 0.00326910731382668, 0.011703277938067913, -0.007917664013803005, 0.01511725876480341, -0.01903514936566353, 0.03822147101163864, -0.00656341016292572, 0.007804284803569317, 0.05321275070309639, 0.033938247710466385, 0.06364366412162781, -0.020131150260567665, 0.02181924320757389, -0.005454810801893473, 0.042051177471876144, 0.02297823317348957, 0.020899610593914986, -0.005221753381192684, -0.011974128894507885, -0.03534919023513794, -0.014777121134102345, -0.01511725876480341, 0.05754636600613594, -0.02000517211854458, -0.03985917195677757, 0.009221527725458145, 0.018518641591072083, 0.02955424226820469, 0.012900060974061489, -0.034316178411245346, 0.026102468371391296, -0.034139811992645264, 0.004938304424285889, -0.026329226791858673, 0.028949551284313202, -0.017800573259592056, -0.027941733598709106, -0.00540442019701004, 0.020647656172513962, 0.00020451605087146163, -0.02578752487897873, -0.025636352598667145, 0.004254878498613834, 0.005032787565141916, -0.010657668113708496, 0.03411461412906647, -0.011098587885499, 0.02577492594718933, -0.005054833367466927, -0.007703503128141165, -0.0070295254699885845, 0.004491085652261972, -0.013870085589587688, 0.006075248587876558, 0.034341372549533844, -0.035374388098716736, 0.040111128240823746, 0.006396490149199963, -0.008112928830087185, 0.03214937075972557, 0.02559855952858925, -0.003010854125022888, 0.000424385565565899, 0.009864011779427528, 0.015344018116593361, -0.016162870451807976, -0.011211967095732689, -0.0057792020961642265, 0.007665710058063269, -0.008749113418161869, 0.001475507509894669, -0.03504684567451477, 0.014399189502000809, 0.0015014902455732226, 0.009397896006703377, -0.023116808384656906, -0.006979134865105152, 0.023519935086369514, 0.017447836697101593, 0.04779573529958725, -0.01342916488647461, -0.03411461412906647, 0.0279921256005764, 0.04721624031662941, -0.025900904089212418, -0.0024455315433442593, 0.013114222325384617, 0.007344468496739864, 0.01796434260904789, -0.04008593037724495, 0.014953488484025002, -0.04492345452308655, -0.008194814436137676, 0.024187615141272545, -0.0009778976673260331, 0.0073381694965064526, 0.036835722625255585, -0.03164546191692352, 0.020647656172513962, 0.0036911307834088802, 0.015784937888383865, 0.029201505705714226, -0.00021357065998017788, 0.01814071089029312, 0.0054705580696463585, -0.02297823317348957, -0.024943478405475616, -0.014663740992546082, 0.03293043002486229, -0.0003143523936159909, 0.0003238006611354649, 0.019765816628932953, 0.010708058252930641, 0.007501939777284861, -0.021365726366639137, 0.05250728130340576, -0.04280703887343407, -0.032577693462371826, -0.015621167607605457, 0.04318496957421303, -0.007892468944191933, -0.00931601133197546, -0.008919183164834976, -0.00312423356808722, 0.012377255596220493, -0.01341656781733036, 0.0018298182403668761, -0.04036308079957962, 0.01167808286845684, 0.012503232806921005, -0.0022738876286894083, -0.00610359339043498, -0.005448512267321348, 0.01825409010052681, 0.018619423732161522, 0.008629435673356056, -0.026681961491703987, -0.012767785228788853, 0.013215004466474056, 0.012988245114684105, -0.030965186655521393, -0.009253022260963917, -0.011709577403962612, 0.04459591582417488, -0.014663740992546082, 0.01564636267721653, -0.021390922367572784, -0.04086698964238167, -0.020723242312669754, -0.025044258683919907, -0.018581630662083626, -0.010248241946101189, 0.031418703496456146, -0.014386591501533985, 0.0017148640472441912, 0.02809290587902069, 0.021504301577806473, 0.01661638729274273, 0.00364388944581151, -0.03560114651918411, -0.018380068242549896, -0.006607502233237028, 0.0048091779462993145, 0.009234125725924969, 0.007438951171934605, 0.004626511130481958, 0.022134186699986458, 0.01371891237795353, 0.0037478203885257244, 0.032476913183927536, -0.012711095623672009, -0.02597649022936821, 0.0014676338760182261, -0.0001343428302789107, -0.012698497623205185, 0.032023392617702484, -0.007590123917907476, -0.016087282449007034, 0.032779257744550705, -0.010021482594311237, 0.018342275172472, 0.03613024950027466, 0.02829447016119957, -0.03456813097000122, -0.026127662509679794, -0.014462177641689777, 0.03167065605521202, -0.002842359710484743, -0.01958944834768772, 0.00197784136980772, -0.010178954340517521, -0.0006873628590255976, 0.01942567713558674, -0.04328575357794762, 0.005498902872204781, -0.002692761830985546, -0.028999941423535347, -0.004226533696055412, -0.04459591582417488, -0.027160676196217537, -0.00015274730685632676, -0.009101849980652332, -0.008768009953200817, 0.01043720729649067, 0.01814071089029312, -0.003445475362241268, 0.01805252768099308, -0.005275293719023466, -0.0007905066595412791, 0.026127662509679794, 0.026581181213259697, 0.019929585978388786, 0.02261289954185486, 0.06908587366342545, -0.015507788397371769, -0.00931601133197546, -0.029982563108205795, 0.011961530894041061, -0.019375286996364594, 0.02229795791208744, 0.004692649003118277, 0.0014723580097779632, -0.026782743632793427, 0.0115836001932621, -0.01698172092437744, 0.0028675550129264593, 0.0012479611905291677, -0.030763622373342514, -0.03051166795194149, 0.005152465775609016, -0.021592484787106514, -0.010210448876023293, -0.001414093654602766, 0.0231923945248127, 0.022650692611932755, 0.018304482102394104, 0.004698948003351688, -0.03146909549832344, -0.009498678147792816, -0.016780158504843712, -0.0031793485395610332, 0.009391597472131252, -0.015192845836281776, -0.011904841288924217, -0.017057307064533234, 0.03446735069155693, -0.01612507551908493, 0.021894831210374832, 0.013870085589587688, 0.04724143445491791, -0.028798379004001617, 0.01650300808250904, -0.007602721452713013, 0.027765365317463875, 0.03602946922183037, -0.013063831254839897, -0.03870018199086189, -0.008220009505748749, -0.01864461973309517, -0.003163601504638791, -0.044570717960596085, -0.06127529218792915, 0.001878634444437921, -0.027362238615751266, 0.002023508073762059, 0.01816590689122677, 0.0255733635276556, 0.004472189117223024, -0.013303187675774097, 0.028269274160265923, -0.0072562843561172485, -0.005555592942982912, 0.009586862288415432, 0.017170686274766922, -0.02529621310532093, -0.004761936608701944, -0.007986951619386673, 0.02103818580508232, -0.032880038022994995, -0.00039682805072516203, 0.018014734610915184, -0.009731736034154892, 0.009435689076781273, 0.0328044518828392, -0.008642032742500305, -0.017447836697101593, -0.021554691717028618, -0.022738877683877945, -0.012213485315442085, 0.007413755636662245, 0.014991281554102898, -0.01014745980501175, 0.02627883478999138, -0.017939146608114243, -0.005130419973284006, 0.020244529470801353, -0.0015353466151282191, 0.002007761038839817, -0.01583532802760601, -0.0005735897575505078, -0.013643326237797737, -0.029201505705714226, -0.00384860229678452, 0.0026360719930380583, 0.010386817157268524, -0.03391304984688759, -0.016591191291809082, -0.01883358508348465, 0.01365592423826456, 0.00975063256919384, 0.014663740992546082, 0.014260614290833473, 0.019450873136520386, -0.0028533826116472483, 0.000473201711429283, -0.009523873217403889, 0.022638095542788506, -0.012509532272815704, 0.021592484787106514, -0.0014723580097779632, -0.006364996079355478, 0.0028407848440110683, -0.013970866799354553, 0.0015975479036569595, 0.021416116505861282, -0.01138833537697792, 0.04401642084121704, 0.032098978757858276, -0.024502556771039963, 0.021214554086327553, 0.005206006113439798, -0.01727146841585636, 0.02232315205037594, 0.03446735069155693, 0.02375929243862629, -0.013466957956552505, -0.014978684484958649, 0.011174174025654793, 0.023696303367614746, 0.029705414548516273, -0.0013109497958794236, 0.009826218709349632, 0.008213710971176624, -0.028319666162133217, 0.02625364065170288, 0.022285358980298042, -0.0051083737052977085, -0.019853999838232994, -0.0074641467072069645, -0.002888026414439082, 0.001810921705327928, 0.003858050564303994, 0.014625947922468185, -0.023343566805124283, -0.009001067839562893, 0.020685449242591858, 0.029604632407426834, 0.018115514889359474, 0.03126753121614456, -0.011130082421004772, -0.002769922837615013, 0.001908553997054696, 0.01042461022734642, -0.01264180801808834, -0.011463921517133713, 0.009536471217870712, 0.008440469391644001, -0.001729036564938724, -0.04046386480331421, 0.011130082421004772, -0.008163319900631905, -0.020206736400723457, -0.018493447452783585, -0.04525099694728851, 0.01510466169565916, -0.009681344963610172, 0.004783982411026955, -0.010235643945634365, 0.0008542826399207115, 0.002669141162186861, 0.015999099239706993, -0.0011314323637634516, -0.019677631556987762, -0.01270479615777731, -0.036634158343076706, -0.00451313192024827, 0.007930262014269829, 0.0012920532608404756, -0.023041222244501114, -0.012150497175753117, 0.0463344007730484, -0.019287103787064552, -0.015293627046048641, 0.0197910126298666, 0.011715875938534737, 0.051902588456869125, -0.00888139009475708, 0.011520611122250557, -0.007741296663880348, -0.002865980379283428, -0.0048186262138187885, 0.014021257869899273, -0.002360497135668993, -0.021768853068351746, 0.004960350692272186, 0.0032754060812294483, 0.028823575004935265, -0.00714290514588356, 0.016062088310718536, -0.013101624324917793, 0.028143297880887985, 0.020685449242591858, -0.007520836312323809, 0.02212158963084221, 0.008660929277539253, -0.040136322379112244, 0.030839208513498306, 0.01071435771882534, -0.010947415605187416, 0.005385523661971092, 0.019488666206598282, 0.022184578701853752, -0.004122602753341198, -0.03444215655326843, -0.020622460171580315, 0.008049940690398216, 0.008919183164834976, 0.032401327043771744, 0.014903098344802856, -0.0031856473069638014, 0.08465664833784103, -0.019564252346754074, -0.0004511557053774595, 0.015721948817372322, 0.017750181257724762, -0.0041572460904717445, -0.005618581548333168, -0.024678925052285194, -0.0103742191568017, 0.010071873664855957, 0.0042202346958220005, 0.01225757785141468, 0.013744108378887177, -0.002456554677337408, -0.004554074257612228, -0.0021557840518653393, -0.001625105389393866, 0.0059240758419036865, 0.021668070927262306, 0.029277091845870018, 0.016150271520018578, -0.010298633016645908, 0.031141553074121475, -0.013844889588654041, -0.014613350853323936, 0.01056948397308588, -0.0019274505320936441, -0.008453067392110825, -0.00653821462765336, -0.014840109273791313, 0.023582924157381058, 0.03696169704198837, 0.05185220018029213, -0.014499970711767673, -0.019627241417765617, 0.0038958436343818903, 0.006645295303314924, -0.027085090056061745, 0.016175467520952225, 0.034794893115758896, 0.00936010293662548, 0.004327315371483564, -0.02731184847652912, -0.020105954259634018, -0.007073617540299892, -0.03167065605521202, 0.009933299385011196, 0.02685832977294922, 0.008786906488239765, -0.01114897895604372, 0.01902255043387413, -0.0055524432100355625, 0.037037283182144165, -0.0003915134002454579, 0.015671558678150177, 0.012622911483049393, 0.016288846731185913, -0.005013891030102968, -0.04593127220869064, 0.027664585039019585, 0.03333355486392975, 0.015898317098617554, 0.012093807570636272, 0.000699173251632601, 0.015180247835814953, -0.0173722505569458, 0.005930374842137098, -0.05165063589811325, -0.018619423732161522, 0.006229570601135492, -0.04179922118782997, -0.02519543282687664, 0.015873122960329056, -0.018392665311694145, -0.008358584716916084, -0.04001034423708916, 0.019551655277609825, 0.01766199804842472, 0.000210224388865754, -0.0032943026162683964, -0.03912850469350815, 0.02771497517824173, -0.023406555876135826, -0.01365592423826456, 0.00550205260515213, 0.009089251980185509, -0.018090320751070976, 0.0020471287425607443, 0.01516764983534813, -0.009366401471197605, 0.02209639362990856, -0.02425060234963894, -0.04315977543592453, -0.002878578146919608, 0.03890174627304077, -0.012566221877932549, 0.001677070977166295, -0.0086861252784729, -0.003237613011151552, 0.040136322379112244, 0.05069321021437645, -0.004135200288146734, 0.001371576334349811, 0.039733197540044785, -0.030385691672563553, -0.01312682032585144, 0.012484336271882057, -0.04782092943787575, -0.0026407961267977953, 0.02472931705415249, -0.02152949571609497, 0.00363129167817533, -0.006676789373159409, 0.008188514970242977, 0.027840951457619667, -0.0005027275765314698, -0.03834744915366173, 0.013441762886941433, -0.013265394605696201, -0.031620267778635025, -0.002311680931597948, 0.0027903940062969923, -0.034895673394203186, 0.024263201281428337, -0.003160452004522085, 0.011955232359468937, -0.0027809457387775183, -0.003105337033048272, 0.0024455315433442593, -0.043411727994680405, 0.010928518138825893, 0.005763454828411341, -0.01249693427234888, -0.011696979403495789, -0.033358752727508545, -0.013983464799821377, -0.004358809906989336, 0.005013891030102968, -0.0005519374390132725, 0.013152015395462513, -0.007413755636662245, -0.011356840841472149, 0.003961981739848852, -0.00907035544514656, -0.023305773735046387, -0.018783194944262505, 0.04434395954012871, 0.01621326059103012, -0.011501714587211609, 0.027211066335439682, -0.022272761911153793, 0.019438276067376137, 0.030486471951007843, -0.007596422918140888, 0.021390922367572784, 0.032023392617702484, -0.006544513627886772, 0.0207484383136034, 0.015583374537527561, 0.023028625175356865, 0.01540700625628233, 0.055681902915239334, 0.010349024087190628, -0.005804397631436586, 0.017750181257724762, -0.04683830961585045, -0.01235206052660942, 0.022247565910220146, -0.0008385354885831475, -0.021201955154538155, 0.037037283182144165, -0.004484786652028561]\n"
     ]
    }
   ],
   "source": [
    "embedding = utils.get_chunk_embeddings(chunks[7], \"text-embedding-3-small\")\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = pd.DataFrame({\"chunks\": chunks})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "database[\"embeddings\"] = database[\"chunks\"].apply(\n",
    "    lambda x: np.array(utils.get_chunk_embeddings(x, \"text-embedding-3-small\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunks</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Minneapolis[a] is a city in and the county sea...</td>\n",
       "      <td>[-0.03441869467496872, -0.040999993681907654, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[14] Minneapolis is built on an artesian aquif...</td>\n",
       "      <td>[-0.008452524431049824, 0.011000331491231918, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dakota people originally inhabited the site of...</td>\n",
       "      <td>[0.0007054487941786647, -0.0065779597498476505...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>For a time in the 19th century, Minneapolis wa...</td>\n",
       "      <td>[-0.042372457683086395, -0.002427722094580531,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The city's major arts institutions include the...</td>\n",
       "      <td>[-0.002938700607046485, -0.010737665928900242,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Residents adhere to more than fifty religions....</td>\n",
       "      <td>[-0.020965415984392166, 0.006626776419579983, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Two Indigenous nations inhabited the area now ...</td>\n",
       "      <td>[0.042408354580402374, -0.020726656541228294, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Dakota are the only inhabitants of the Minneap...</td>\n",
       "      <td>[0.01706407591700554, -0.008185463026165962, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>In the space of sixty years, the US seized all...</td>\n",
       "      <td>[0.017029542475938797, -0.010994214564561844, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[b] Pike bought a 9-square-mile (23 km2) strip...</td>\n",
       "      <td>[0.007397182751446962, -0.022310152649879456, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[37] Under pressure from US officials[38] in a...</td>\n",
       "      <td>[0.014520539902150631, -0.0036461693234741688,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[52] At the beginning of the American Civil Wa...</td>\n",
       "      <td>[0.006639148574322462, 0.006992733106017113, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[58] The war went on for six weeks in the Minn...</td>\n",
       "      <td>[-0.013272798620164394, 0.028846044093370438, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[79] With Governor Alexander Ramsey calling fo...</td>\n",
       "      <td>[0.0015017843106761575, 0.023696845397353172, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>While the Dakota were being expelled, Franklin...</td>\n",
       "      <td>[0.057728275656700134, -0.023256691172719002, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>In 1851, after a meeting of the Minnesota Terr...</td>\n",
       "      <td>[-0.016625264659523964, -0.052147991955280304,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Minneapolis originated around a source of ener...</td>\n",
       "      <td>[-0.032446231693029404, -0.001552674570120871,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[97] Through its expanding mill industries, Mi...</td>\n",
       "      <td>[0.013883381150662899, 0.025766069069504738, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Disasters struck in the late 19th century: the...</td>\n",
       "      <td>[-0.01751815527677536, 0.018691303208470345, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>The lumber industry was built around forests i...</td>\n",
       "      <td>[-0.015324199572205544, 0.04624434560537338, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[107] Auxiliary businesses on the river's west...</td>\n",
       "      <td>[0.007507123053073883, 0.02040758915245533, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[110] Growing use of steam power freed lumberm...</td>\n",
       "      <td>[-0.0065580978989601135, 0.016273798421025276,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Seymour Cray and colleagues began work on the ...</td>\n",
       "      <td>[-0.010859242640435696, -0.021557409316301346,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>In 1877, Cadwallader C.  Washburn co-founded W...</td>\n",
       "      <td>[-0.0034697633236646652, -0.001842176890932023...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Pillsbury Company across the river hired Washb...</td>\n",
       "      <td>[-0.03057912550866604, -0.009972292929887772, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[123] Decades of soil exhaustion, stem rust, a...</td>\n",
       "      <td>[-0.006807503290474415, -0.013357703574001789,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               chunks  \\\n",
       "0   Minneapolis[a] is a city in and the county sea...   \n",
       "1   [14] Minneapolis is built on an artesian aquif...   \n",
       "2   Dakota people originally inhabited the site of...   \n",
       "3   For a time in the 19th century, Minneapolis wa...   \n",
       "4   The city's major arts institutions include the...   \n",
       "5   Residents adhere to more than fifty religions....   \n",
       "6   Two Indigenous nations inhabited the area now ...   \n",
       "7   Dakota are the only inhabitants of the Minneap...   \n",
       "8   In the space of sixty years, the US seized all...   \n",
       "9   [b] Pike bought a 9-square-mile (23 km2) strip...   \n",
       "10  [37] Under pressure from US officials[38] in a...   \n",
       "11  [52] At the beginning of the American Civil Wa...   \n",
       "12  [58] The war went on for six weeks in the Minn...   \n",
       "13  [79] With Governor Alexander Ramsey calling fo...   \n",
       "14  While the Dakota were being expelled, Franklin...   \n",
       "15  In 1851, after a meeting of the Minnesota Terr...   \n",
       "16  Minneapolis originated around a source of ener...   \n",
       "17  [97] Through its expanding mill industries, Mi...   \n",
       "18  Disasters struck in the late 19th century: the...   \n",
       "19  The lumber industry was built around forests i...   \n",
       "20  [107] Auxiliary businesses on the river's west...   \n",
       "21  [110] Growing use of steam power freed lumberm...   \n",
       "22  Seymour Cray and colleagues began work on the ...   \n",
       "23  In 1877, Cadwallader C.  Washburn co-founded W...   \n",
       "24  Pillsbury Company across the river hired Washb...   \n",
       "25  [123] Decades of soil exhaustion, stem rust, a...   \n",
       "\n",
       "                                           embeddings  \n",
       "0   [-0.03441869467496872, -0.040999993681907654, ...  \n",
       "1   [-0.008452524431049824, 0.011000331491231918, ...  \n",
       "2   [0.0007054487941786647, -0.0065779597498476505...  \n",
       "3   [-0.042372457683086395, -0.002427722094580531,...  \n",
       "4   [-0.002938700607046485, -0.010737665928900242,...  \n",
       "5   [-0.020965415984392166, 0.006626776419579983, ...  \n",
       "6   [0.042408354580402374, -0.020726656541228294, ...  \n",
       "7   [0.01706407591700554, -0.008185463026165962, 0...  \n",
       "8   [0.017029542475938797, -0.010994214564561844, ...  \n",
       "9   [0.007397182751446962, -0.022310152649879456, ...  \n",
       "10  [0.014520539902150631, -0.0036461693234741688,...  \n",
       "11  [0.006639148574322462, 0.006992733106017113, 0...  \n",
       "12  [-0.013272798620164394, 0.028846044093370438, ...  \n",
       "13  [0.0015017843106761575, 0.023696845397353172, ...  \n",
       "14  [0.057728275656700134, -0.023256691172719002, ...  \n",
       "15  [-0.016625264659523964, -0.052147991955280304,...  \n",
       "16  [-0.032446231693029404, -0.001552674570120871,...  \n",
       "17  [0.013883381150662899, 0.025766069069504738, 0...  \n",
       "18  [-0.01751815527677536, 0.018691303208470345, 0...  \n",
       "19  [-0.015324199572205544, 0.04624434560537338, 0...  \n",
       "20  [0.007507123053073883, 0.02040758915245533, 0....  \n",
       "21  [-0.0065580978989601135, 0.016273798421025276,...  \n",
       "22  [-0.010859242640435696, -0.021557409316301346,...  \n",
       "23  [-0.0034697633236646652, -0.001842176890932023...  \n",
       "24  [-0.03057912550866604, -0.009972292929887772, ...  \n",
       "25  [-0.006807503290474415, -0.013357703574001789,...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final step: retrieve the chunks and generate the final response\n",
    "\n",
    "For that we use KNN algorithms (it can be brute force or some approximation like HNSW) to retrieve the most similar chunks to the user query and then use a generative model to generate the final response using the retrieved chunks as context. In our example, we'll to it using Scikit-learn's KNN. \n",
    "\n",
    "But there are databases (both open source and commercial; and both relational and non-relational) that can do that for you in a fast and efficient way, like PostgreSQL's PGVector, MongoDB, Elastic Search, OpenSearch and RediSearch. And if you don't want to maintain your own database, there are vector storage services like Pinecone, Milvus, TurboPuffer, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is the terrain and climate characteristics of Minneapolis?\"\n",
    "result = utils.vector_sim_retriever(database, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     [14] Minneapolis is built on an artesian aquif...\n",
       "5     Residents adhere to more than fifty religions....\n",
       "2     Dakota people originally inhabited the site of...\n",
       "4     The city's major arts institutions include the...\n",
       "16    Minneapolis originated around a source of ener...\n",
       "Name: chunks, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"chunks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To address your question accurately, it does not pertain to database queries or data retrieval. Instead, you are asking for geographical and meteorological information about Minneapolis. \n",
      "\n",
      "Minneapolis is characterized by:\n",
      "\n",
      "**Terrain:**\n",
      "- The terrain of Minneapolis includes flatlands with some rolling hills. The city is located in the River Valleys region, particularly near the banks of the Mississippi River, which contributes to its varied topography.\n",
      "- There are also several lakes within the city, such as Lake Calhoun (Bde Maka Ska) and Lake Harriet, which provide scenic areas and recreational opportunities.\n",
      "\n",
      "**Climate:**\n",
      "- Minneapolis has a humid continental climate, marked by four distinct seasons.\n",
      "- Winters (December to February) are cold, with average temperatures often falling below freezing and substantial snowfall.\n",
      "- Summers (June to August) are warm to hot, with humidity and temperatures frequently reaching the 80s°F (around 26-34°C) and sometimes exceeding the 90s°F (above 37°C).\n",
      "- Spring and fall seasons offer transitional weather, with moderate temperatures and varying precipitation.\n",
      "\n",
      "If you have a specific data-related query about Minneapolis that fits into a database retrieval context, feel free to provide it, and I'll be glad to help you with that!\n"
     ]
    }
   ],
   "source": [
    "# Generation without RAG:\n",
    "\n",
    "system_message_1 = \"You are a helpful assistant.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": query},\n",
    "]\n",
    "\n",
    "response = utils.get_chat_completionprompt(MODEL, messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minneapolis is characterized by flat terrain and is built on an artesian aquifer. The climate features cold, snowy winters and hot, humid summers, making it a region with notable seasonal variation.\n",
      "\n",
      "**Citation**: [1] Minneapolis is built on an artesian aquifer on flat terrain and is known for cold, snowy winters and hot, humid summers. Nicknamed the \"City of Lakes\", Minneapolis is abundant in water, with thirteen lakes, wetlands, the Mississippi River, creeks, and waterfalls. The city's public park system is connected by the Grand Rounds National Scenic Byway.\n"
     ]
    }
   ],
   "source": [
    "# Generation without RAG:\n",
    "\n",
    "system_message_1 = \"You are a helpful assistant. Here is some information that might help with answering questions:\"\n",
    "contextual_information = \"\\n\".join(\n",
    "    [f\"[{index+1}] \" + chunk for index, chunk in enumerate(result[\"chunks\"])]\n",
    ")\n",
    "system_message_2 = \"With this information in mind, please answer the following question, providing the reference information right after the answer as a complete citation:\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"system\", \"content\": contextual_information},\n",
    "    {\"role\": \"system\", \"content\": system_message_2},\n",
    "    {\"role\": \"user\", \"content\": query},\n",
    "]\n",
    "\n",
    "response = utils.get_chat_completionprompt(MODEL, messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Use the practices highlighted above to create a function that has the user query as the input and outputs a final response. The task is to answer a question about some period of history in a country. Feel free to chose another subject that is as difficult as this one, if you want, i.e., another topic that a RAG would improve the performance of the model compared to a zero-shot approach. This exercise includes creating conversational prompts to solve the problem in multiple steps and also provide the reference used in the answer.\n",
    "\n",
    "**Homework:** Create a more complete database of texts and manually evaluate the performance of your function, try making small changes to the prompt and chosen model to improve it.\n",
    "\n",
    "**Challenge:** Refactor your previous code and use an actual Vector Database of your choice to solve the problem. Also, make your function to work as an API, so you can query it from a web interface. Feel free to use any framework you want, like FastAPI, Flask, Django, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
